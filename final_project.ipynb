{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7BOqG2HAqhD"
      },
      "source": [
        "# Using Satellite Imagery and Machine Learning for Urban Heat Risk Analysis in San Francisco\n",
        "\n",
        "> Background\n",
        "\n",
        "    Urban heat risk is a growing concern in many cities around the world, including San Francisco. The rapid urbanization and industrialization of San Francisco. have led to the emergence of urban heat islands, characterized by higher temperatures in densely built-up areas compared to surrounding suburban areas. This phenomenon poses significant health risks to residents, especially during heat waves, and can exacerbate existing socioeconomic inequalities.\n",
        "\n",
        "> Problem Statement\n",
        "\n",
        "    The main objective of this project is to assess the urban heat risk in San Francisco. over the past 5 years using satellite imagery and machine learning. The analysis will identify areas with high heat risk and help city planners and policymakers implement targeted interventions to reduce heat exposure, particularly for vulnerable populations.\n",
        "\n",
        "> Dataset\n",
        "\n",
        "    For this project, I will use publicly available Landsat 8 satellite imagery through the Google Earth Engine of San Francisco in 2020. The Landsat 8 images provide high-resolution (30-meter) data in multiple spectral bands, including the thermal infrared band, which is essential for calculating land surface temperature. Then I use Python script that utilizes the Google Earth Engine (GEE) Python API to download Landsat 8 satellite imagery for the year 2020 over San Francisco with less than 5% cloud cover. The script loops through each image in the resulting image collection, exports them as GeoTIFF files, and uploads them to a Google Cloud Storage bucket named â€˜sf_imageryâ€™. Moreover, GeoTIFF images can be extracted from the specified Google Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwQ6ucDeiM_T",
        "outputId": "c69829be-c373-49a3-fbaa-3aa292e34278"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting states\n",
            "  Downloading states-0.2.1.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: states\n",
            "  Building wheel for states (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for states: filename=states-0.2.1-py3-none-any.whl size=10782 sha256=a16c74d9c701a8baa3aceab1016fd1a6d8ac13f8e21db69504996f452c4acdc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e0/5b/da/e5df3bd1cce7f8d5a50f706c3facd42345a3c42416eef662e9\n",
            "Successfully built states\n",
            "Installing collected packages: states\n",
            "Successfully installed states-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package Import"
      ],
      "metadata": {
        "id": "zbNmpTQkX-Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import zipfile\n",
        "import rasterio\n",
        "from rasterio.features import geometry_mask\n",
        "from rasterio.transform import from_bounds\n",
        "from io import BytesIO\n",
        "from shapely.geometry import box, Point\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Census data\n",
        "import census\n",
        "from us import states\n",
        "\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='sfurbanheatisland')\n"
      ],
      "metadata": {
        "id": "ptNVB5uOMTzt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Earth Engine\n",
        "ee.Initialize(project='sfurbanheatisland')  # Update with YOUR project name\n"
      ],
      "metadata": {
        "id": "fSPT8BjCMQrn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLFZiKnnAqhF"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this section, it generates a set of satellite-derived vegetation indices including Normalized Difference Vegetation Index (NDVI), Normalized Difference Built-Up Index (NDBI), Normalized Difference Water Index (NDWI), Built-Up Index (BU), Enhanced Vegetation Index (EVI), and Soil-Adjusted Vegetation Index (SAVI) for a region of interest in San Francisco.\n",
        "\n",
        "The code first defines the region of San Francisco and filters the Landsat-8 image collection to get the image on September 30th, 2022, with less than 20% cloud cover. It then calculates the median of the filtered image collection and computes the vegetation indices using the normalized difference or expression methods. The vegetation indices are clipped to the region of interest, and then added to a Folium map, along with the San Francisco neighborhood boundaries as a GeoJSON layer. The final output is an interactive map that displays the vegetation indices for the selected area."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\" \" * 10 + \"URBAN HEAT VULNERABILITY ANALYSIS\")\n",
        "print(\" \" * 15 + \"Temporal Prediction & Demographics\")\n",
        "print(\" \" * 25 + \"New York City\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "OUTPUT_DIR = \"./nyc_heat_vulnerability_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# NYC Bounding Box\n",
        "NYC_BOUNDS = {\n",
        "    'west': -74.3,\n",
        "    'south': 40.5,\n",
        "    'east': -73.65,\n",
        "    'north': 40.95\n",
        "}\n",
        "\n",
        "# Temporal parameters\n",
        "TRAIN_YEAR = 2024\n",
        "PREDICT_YEAR = 2025\n",
        "TRAIN_MONTHS = ('06-01', '08-31')  # Summer\n",
        "PREDICT_MONTHS = ('06-01', '08-31')  # Summer\n",
        "\n",
        "# ML parameters\n",
        "RANDOM_SEED = 42\n",
        "SAMPLE_SIZE = 30000  # Subsample for manageable computation\n",
        "HIGH_RISK_PERCENTILE = 80\n",
        "\n",
        "CENSUS_API_KEY = \"a9e713a06a0a0f8ec8531e047c9d01e7d9f507d9\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ALcq5lXCsMG",
        "outputId": "1ed1083d-e72c-49c0-bcca-fc7e138eb66e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "          URBAN HEAT VULNERABILITY ANALYSIS\n",
            "               Temporal Prediction & Demographics\n",
            "                         New York City\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up"
      ],
      "metadata": {
        "id": "HVH6lncimXcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_landsat(year, months, bounds, description=\"\"):\n",
        "    \"\"\"Download Landsat imagery for specified year and months\"\"\"\n",
        "\n",
        "    print(f\"\\n    Downloading {description} ({year})...\")\n",
        "\n",
        "    region = ee.Geometry.Rectangle([bounds[0], bounds[1], bounds[2], bounds[3]])\n",
        "\n",
        "    start_date = f\"{year}-{months[0]}\"\n",
        "    end_date = f\"{year}-{months[1]}\"\n",
        "\n",
        "    # Get Landsat imagery (9 preferred, then 8)\n",
        "    landsat = ee.ImageCollection('LANDSAT/LC09/C02/T1_L2') \\\n",
        "        .merge(ee.ImageCollection('LANDSAT/LC08/C02/T1_L2')) \\\n",
        "        .filterBounds(region) \\\n",
        "        .filterDate(start_date, end_date) \\\n",
        "        .filter(ee.Filter.lt('CLOUD_COVER', 20)) \\\n",
        "        .sort('CLOUD_COVER')\n",
        "\n",
        "    count = landsat.size().getInfo()\n",
        "\n",
        "    if count == 0:\n",
        "        print(f\"      âš  No images found for {year}\")\n",
        "        return None\n",
        "\n",
        "    # Get best image\n",
        "    image = landsat.first()\n",
        "    img_info = image.getInfo()\n",
        "\n",
        "    print(f\"      Date: {img_info['properties']['DATE_ACQUIRED']}\")\n",
        "    print(f\"      Cloud: {img_info['properties']['CLOUD_COVER']:.1f}%\")\n",
        "\n",
        "    # Download bands\n",
        "    bands = ['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'ST_B10']\n",
        "    image = image.select(bands)\n",
        "\n",
        "    scale = 90  # 90m resolution\n",
        "\n",
        "    download_url = image.getDownloadURL({\n",
        "        'region': region.coordinates().getInfo(),\n",
        "        'scale': scale,\n",
        "        'format': 'GEO_TIFF',\n",
        "        'bands': bands\n",
        "    })\n",
        "\n",
        "    response = requests.get(download_url)\n",
        "\n",
        "    # Handle redirect\n",
        "    try:\n",
        "        json_response = response.json()\n",
        "        if 'downloadUrl' in json_response:\n",
        "            response = requests.get(json_response['downloadUrl'])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Process downloaded file\n",
        "    if not response.content[:4] == b'PK\\x03\\x04':\n",
        "        with rasterio.open(BytesIO(response.content)) as src:\n",
        "            img_arrays = {}\n",
        "            for i, band in enumerate(bands, start=1):\n",
        "                if i <= src.count:\n",
        "                    img_arrays[band] = src.read(i)\n",
        "    else:\n",
        "        img_arrays = {}\n",
        "        with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
        "            for tiff_file in [f for f in z.namelist() if f.endswith('.tif')]:\n",
        "                parts = tiff_file.split('.')\n",
        "                band_name = parts[-2] if len(parts) >= 2 else tiff_file.replace('.tif', '')\n",
        "                with z.open(tiff_file) as f:\n",
        "                    with rasterio.open(BytesIO(f.read())) as src:\n",
        "                        img_arrays[band_name] = src.read(1)\n",
        "\n",
        "    img = np.stack([img_arrays[b] for b in bands], axis=0)\n",
        "\n",
        "    print(f\"      âœ“ Downloaded: {img.shape}\")\n",
        "\n",
        "    return img, img_info\n",
        "\n",
        "def preprocess_landsat(img):\n",
        "    \"\"\"Apply Landsat Collection 2 scaling\"\"\"\n",
        "\n",
        "    img = img.astype(float)\n",
        "    img[img <= 0] = np.nan\n",
        "\n",
        "    # Apply scaling\n",
        "    for i in range(6):  # Optical bands\n",
        "        img[i] = img[i] * 0.0000275 - 0.2\n",
        "\n",
        "    img[6] = img[6] * 0.00341802 + 149.0  # Thermal to Kelvin\n",
        "    img[6] = img[6] - 273.15  # Convert to Celsius\n",
        "\n",
        "    # Clip optical\n",
        "    for i in range(6):\n",
        "        img[i] = np.clip(img[i], 0, 1)\n",
        "\n",
        "    return img\n",
        "\n",
        "def calculate_indices(img):\n",
        "    \"\"\"Calculate spectral indices\"\"\"\n",
        "\n",
        "    blue, green, red, nir, swir1, swir2, LST = img\n",
        "\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        NDVI = (nir - red) / (nir + red)\n",
        "        NDBI = (swir1 - nir) / (swir1 + nir)\n",
        "        NDWI = (nir - swir1) / (nir + swir1)\n",
        "        MNDWI = (green - swir1) / (green + swir1)\n",
        "        BU = NDBI - NDVI\n",
        "        UI = (swir2 - nir) / (swir2 + nir)\n",
        "\n",
        "        # EVI\n",
        "        G, C1, C2, L = 2.5, 6, 7.5, 1\n",
        "        EVI = G * ((nir - red) / (nir + C1 * red - C2 * blue + L))\n",
        "\n",
        "        # SAVI\n",
        "        L = 0.5\n",
        "        SAVI = ((nir - red) / (nir + red + L)) * (1 + L)\n",
        "\n",
        "        # Brightness\n",
        "        Brightness = (blue + green + red + nir + swir1 + swir2) / 6\n",
        "\n",
        "    indices = {\n",
        "        'NDVI': NDVI,\n",
        "        'NDBI': NDBI,\n",
        "        'NDWI': NDWI,\n",
        "        'MNDWI': MNDWI,\n",
        "        'BU': BU,\n",
        "        'UI': UI,\n",
        "        'EVI': EVI,\n",
        "        'SAVI': SAVI,\n",
        "        'Brightness': Brightness,\n",
        "        'LST': LST\n",
        "    }\n",
        "\n",
        "    return indices\n",
        "\n",
        "def create_feature_matrix(indices, include_lst=False):\n",
        "    \"\"\"Create feature matrix from indices\"\"\"\n",
        "\n",
        "    feature_names = [k for k in indices.keys() if k != 'LST']\n",
        "\n",
        "    features_list = []\n",
        "    for fname in feature_names:\n",
        "        feat = indices[fname]\n",
        "        feat_filled = np.nan_to_num(feat, nan=np.nanmean(feat[~np.isnan(feat)]))\n",
        "        features_list.append(feat_filled.flatten())\n",
        "\n",
        "    X = np.column_stack(features_list)\n",
        "\n",
        "    if include_lst:\n",
        "        lst = indices['LST']\n",
        "        lst_filled = np.nan_to_num(lst, nan=np.nanmean(lst[~np.isnan(lst)]))\n",
        "        y = lst_filled.flatten()\n",
        "    else:\n",
        "        y = None\n",
        "\n",
        "    # Remove invalid\n",
        "    valid_mask = ~np.isnan(X).any(axis=1) & ~np.isinf(X).any(axis=1)\n",
        "    X = X[valid_mask]\n",
        "    if y is not None:\n",
        "        y = y[valid_mask]\n",
        "\n",
        "    return X, y, feature_names\n",
        "\n",
        "def get_demographic_data():\n",
        "    \"\"\"Get Census demographic data for NYC (simulated for demo)\"\"\"\n",
        "\n",
        "    print(\"\\n    Loading demographic data...\")\n",
        "\n",
        "    # For demo, create synthetic demographic data\n",
        "    # In real analysis, use Census API:\n",
        "    # c = census.Census(CENSUS_API_KEY)\n",
        "    # ny_fips = states.NY.fips\n",
        "    # data = c.acs5.state_county_tract(...)\n",
        "\n",
        "    # Create synthetic census tracts\n",
        "    np.random.seed(42)\n",
        "    n_tracts = 200\n",
        "\n",
        "    # Generate synthetic data correlated with heat risk\n",
        "    demographic_data = pd.DataFrame({\n",
        "        'tract_id': range(n_tracts),\n",
        "        'lon': np.random.uniform(NYC_BOUNDS['west'], NYC_BOUNDS['east'], n_tracts),\n",
        "        'lat': np.random.uniform(NYC_BOUNDS['south'], NYC_BOUNDS['north'], n_tracts),\n",
        "        'median_income': np.random.lognormal(10.8, 0.5, n_tracts),  # Log-normal income\n",
        "        'poverty_rate': np.random.beta(2, 8, n_tracts) * 100,  # Beta distribution for %\n",
        "        'elderly_pct': np.random.beta(3, 10, n_tracts) * 100,  # % over 65\n",
        "        'minority_pct': np.random.beta(6, 4, n_tracts) * 100,  # % minority\n",
        "        'no_ac_pct': np.random.beta(2, 10, n_tracts) * 100,  # % without AC\n",
        "        'population_density': np.random.exponential(10000, n_tracts)  # People per sq km\n",
        "    })\n",
        "\n",
        "    # Create vulnerability index (higher = more vulnerable)\n",
        "    demographic_data['vulnerability_index'] = (\n",
        "        demographic_data['poverty_rate'] * 0.3 +\n",
        "        demographic_data['elderly_pct'] * 0.2 +\n",
        "        demographic_data['minority_pct'] * 0.2 +\n",
        "        demographic_data['no_ac_pct'] * 0.2 +\n",
        "        (100 - demographic_data['median_income'] / demographic_data['median_income'].max() * 100) * 0.1\n",
        "    )\n",
        "\n",
        "    print(f\"      âœ“ Loaded {n_tracts} census tracts\")\n",
        "\n",
        "    return demographic_data"
      ],
      "metadata": {
        "id": "v-R23bYMmVNU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Collection"
      ],
      "metadata": {
        "id": "8-8b56XhYOkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 1: DATA COLLECTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "nyc_bounds = [NYC_BOUNDS['west'], NYC_BOUNDS['south'],\n",
        "              NYC_BOUNDS['east'], NYC_BOUNDS['north']]\n",
        "\n",
        "# Download training data (2024)\n",
        "train_img, train_info = download_landsat(\n",
        "    TRAIN_YEAR, TRAIN_MONTHS, nyc_bounds, \"Training data\"\n",
        ")\n",
        "\n",
        "# Download prediction data (2025 - simulated with 2023 for demo)\n",
        "# Note: Since we're in 2024, we'll use 2023 data to simulate 2025\n",
        "predict_img, predict_info = download_landsat(\n",
        "    2023, PREDICT_MONTHS, nyc_bounds, \"Prediction data (using 2023 as proxy)\"\n",
        ")\n",
        "\n",
        "if train_img is None or predict_img is None:\n",
        "    print(\"\\nâš  Error: Could not download all required imagery\")\n",
        "    print(\"  Please check date ranges and try again\")\n",
        "    exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSApZ8ZlMb7z",
        "outputId": "5220c07d-40c1-462d-945b-4b74abd96c4a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 1: DATA COLLECTION\n",
            "================================================================================\n",
            "\n",
            "    Downloading Training data (2024)...\n",
            "      Date: 2024-07-02\n",
            "      Cloud: 0.4%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:rasterio._env:CPLE_AppDefined in 8660a8cf-64ef-4a92-bd32-2dab5f781cf5.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      âœ“ Downloaded: (7, 563, 618)\n",
            "\n",
            "    Downloading Prediction data (using 2023 as proxy) (2023)...\n",
            "      Date: 2023-08-09\n",
            "      Cloud: 0.4%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:rasterio._env:CPLE_AppDefined in 693373f8-60e4-464d-8c17-4bba271254dd.tif: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
            "WARNING:rasterio._env:CPLE_AppDefined in TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      âœ“ Downloaded: (7, 563, 617)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Preprocessing"
      ],
      "metadata": {
        "id": "spDbmhPsm2Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: PREPROCESSING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n2.1 Applying Landsat scaling...\")\n",
        "\n",
        "train_img = preprocess_landsat(train_img)\n",
        "predict_img = preprocess_landsat(predict_img)\n",
        "\n",
        "print(f\"    Training LST range: {np.nanmin(train_img[6]):.1f}Â°C to {np.nanmax(train_img[6]):.1f}Â°C\")\n",
        "print(f\"    Prediction LST range: {np.nanmin(predict_img[6]):.1f}Â°C to {np.nanmax(predict_img[6]):.1f}Â°C\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiDimOwuaLJR",
        "outputId": "4c767e38-2389-4630-ca83-f3b5b6c1628c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 2: PREPROCESSING\n",
            "================================================================================\n",
            "\n",
            "2.1 Applying Landsat scaling...\n",
            "    Training LST range: 17.6Â°C to 65.5Â°C\n",
            "    Prediction LST range: 11.1Â°C to 56.7Â°C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: FEATURE ENGINEERING"
      ],
      "metadata": {
        "id": "Yla3zH6im7LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: FEATURE ENGINEERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n3.1 Calculating indices...\")\n",
        "\n",
        "train_indices = calculate_indices(train_img)\n",
        "predict_indices = calculate_indices(predict_img)\n",
        "\n",
        "print(\"    âœ“ Calculated 9 indices for both years\")\n",
        "\n",
        "print(\"\\n3.2 Creating feature matrices...\")\n",
        "\n",
        "X_train_full, y_train_full, feature_names = create_feature_matrix(train_indices, include_lst=True)\n",
        "X_predict_full, y_predict_full, _ = create_feature_matrix(predict_indices, include_lst=True)\n",
        "\n",
        "# Subsample for faster computation\n",
        "if len(X_train_full) > SAMPLE_SIZE:\n",
        "    sample_idx = np.random.RandomState(RANDOM_SEED).choice(\n",
        "        len(X_train_full), size=SAMPLE_SIZE, replace=False\n",
        "    )\n",
        "    X_train = X_train_full[sample_idx]\n",
        "    y_train = y_train_full[sample_idx]\n",
        "else:\n",
        "    X_train = X_train_full\n",
        "    y_train = y_train_full\n",
        "\n",
        "if len(X_predict_full) > SAMPLE_SIZE:\n",
        "    sample_idx = np.random.RandomState(RANDOM_SEED).choice(\n",
        "        len(X_predict_full), size=SAMPLE_SIZE, replace=False\n",
        "    )\n",
        "    X_predict = X_predict_full[sample_idx]\n",
        "    y_predict = y_predict_full[sample_idx]\n",
        "else:\n",
        "    X_predict = X_predict_full\n",
        "    y_predict = y_predict_full\n",
        "\n",
        "print(f\"    Training samples: {X_train.shape[0]:,}\")\n",
        "print(f\"    Prediction samples: {X_predict.shape[0]:,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIIWYIIoMlaG",
        "outputId": "a88b7795-8161-4859-a24d-1010c13dcf58"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 3: FEATURE ENGINEERING\n",
            "================================================================================\n",
            "\n",
            "3.1 Calculating indices...\n",
            "    âœ“ Calculated 9 indices for both years\n",
            "\n",
            "3.2 Creating feature matrices...\n",
            "    Training samples: 30,000\n",
            "    Prediction samples: 30,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4: TEMPORAL PREDICTION MODEL"
      ],
      "metadata": {
        "id": "CqJdPLfxYh4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: TEMPORAL PREDICTION MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n4.1 Training models on 2024 data...\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_predict_scaled = scaler.transform(X_predict)\n",
        "\n",
        "# Train models\n",
        "models = {\n",
        "    'Random Forest': RandomForestRegressor(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=20,\n",
        "        random_state=RANDOM_SEED,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n    Training {name}...\")\n",
        "\n",
        "    # Train on 2024\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on 2025 (2023 proxy)\n",
        "    y_pred = model.predict(X_predict)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(y_predict, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_predict, y_pred))\n",
        "    r2 = r2_score(y_predict, y_pred)\n",
        "\n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'predictions': y_pred,\n",
        "        'mae': mae,\n",
        "        'rmse': rmse,\n",
        "        'r2': r2\n",
        "    }\n",
        "\n",
        "    print(f\"      Temporal MAE: {mae:.2f}Â°C\")\n",
        "    print(f\"      Temporal RMSE: {rmse:.2f}Â°C\")\n",
        "    print(f\"      Temporal RÂ²: {r2:.3f}\")\n",
        "\n",
        "# Select best model\n",
        "best_model_name = max(results, key=lambda x: results[x]['r2'])\n",
        "best_model = results[best_model_name]['model']\n",
        "print(f\"\\n    ðŸ† Best temporal model: {best_model_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZIUmfEhRKbi",
        "outputId": "faa4764d-f8f5-4215-b025-0cdfecf30e2b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 4: TEMPORAL PREDICTION MODEL\n",
            "================================================================================\n",
            "\n",
            "4.1 Training models on 2024 data...\n",
            "\n",
            "    Training Random Forest...\n",
            "      Temporal MAE: 2.21Â°C\n",
            "      Temporal RMSE: 2.93Â°C\n",
            "      Temporal RÂ²: 0.839\n",
            "\n",
            "    Training Gradient Boosting...\n",
            "      Temporal MAE: 2.19Â°C\n",
            "      Temporal RMSE: 2.91Â°C\n",
            "      Temporal RÂ²: 0.841\n",
            "\n",
            "    ðŸ† Best temporal model: Gradient Boosting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5: HIGH-RISK AREA IDENTIFICATION"
      ],
      "metadata": {
        "id": "niU8YkfSnqAX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O23gr5tAqhF",
        "outputId": "bd0e0746-a172-4908-9728-1bd6ff226a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 5: HIGH-RISK AREA IDENTIFICATION\n",
            "================================================================================\n",
            "\n",
            "5.1 Identifying high-temperature areas...\n",
            "    High-risk threshold: 42.7Â°C (top 20%)\n",
            "    High-risk pixels: 69,474 (20.0%)\n",
            "\n",
            "5.2 Clustering high-risk areas...\n",
            "    âœ“ Identified 5 high-risk clusters\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 5: HIGH-RISK AREA IDENTIFICATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n5.1 Identifying high-temperature areas...\")\n",
        "\n",
        "# Get full predictions\n",
        "y_pred_full = best_model.predict(X_predict_full)\n",
        "\n",
        "# Define high-risk threshold\n",
        "threshold = np.percentile(y_pred_full, HIGH_RISK_PERCENTILE)\n",
        "high_risk_mask = y_pred_full > threshold\n",
        "\n",
        "print(f\"    High-risk threshold: {threshold:.1f}Â°C (top {100-HIGH_RISK_PERCENTILE}%)\")\n",
        "print(f\"    High-risk pixels: {high_risk_mask.sum():,} ({high_risk_mask.mean()*100:.1f}%)\")\n",
        "\n",
        "# Create spatial high-risk map\n",
        "height, width = predict_img.shape[1], predict_img.shape[2]\n",
        "risk_map = np.zeros(height * width)\n",
        "valid_idx = ~np.isnan(X_predict_full).any(axis=1) & ~np.isinf(X_predict_full).any(axis=1)\n",
        "risk_map[valid_idx] = y_pred_full\n",
        "risk_map = risk_map.reshape(height, width)\n",
        "\n",
        "# Identify clusters\n",
        "print(\"\\n5.2 Clustering high-risk areas...\")\n",
        "\n",
        "high_risk_coords = np.column_stack(np.where(risk_map > threshold))\n",
        "if len(high_risk_coords) > 100:\n",
        "    # Subsample for clustering\n",
        "    sample_coords = high_risk_coords[\n",
        "        np.random.choice(len(high_risk_coords), 100, replace=False)\n",
        "    ]\n",
        "else:\n",
        "    sample_coords = high_risk_coords\n",
        "\n",
        "# K-means clustering\n",
        "n_clusters = min(5, len(sample_coords) // 20)\n",
        "if n_clusters > 0:\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_SEED)\n",
        "    clusters = kmeans.fit_predict(sample_coords)\n",
        "    print(f\"    âœ“ Identified {n_clusters} high-risk clusters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6: DEMOGRAPHIC ANALYSIS\n"
      ],
      "metadata": {
        "id": "AMl1TCMHs0HZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nio4JGVAqhF",
        "outputId": "f4e1e6da-b38c-4ccb-8cbb-7daced397094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 6: DEMOGRAPHIC ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "    Loading demographic data...\n",
            "      âœ“ Loaded 200 census tracts\n",
            "\n",
            "6.1 Analyzing correlation with vulnerability...\n",
            "    Correlation (temperature vs vulnerability): -0.006\n",
            "    P-value: 0.9381\n",
            "    âš  Relationship not statistically significant\n",
            "\n",
            "6.2 Identifying vulnerable hot spots...\n",
            "    Vulnerable areas mean temperature: 35.4Â°C\n",
            "    Overall mean temperature: 35.5Â°C\n",
            "    Temperature difference: -0.2Â°C\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 6: DEMOGRAPHIC ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load demographic data\n",
        "demographic_data = get_demographic_data()\n",
        "\n",
        "print(\"\\n6.1 Analyzing correlation with vulnerability...\")\n",
        "\n",
        "# For each census tract, get average predicted temperature\n",
        "tract_temperatures = []\n",
        "tract_vulnerabilities = []\n",
        "\n",
        "for _, tract in demographic_data.iterrows():\n",
        "    # Find nearest pixel to tract centroid\n",
        "    # (Simplified - in real analysis, use proper spatial join)\n",
        "    lon_idx = int((tract['lon'] - NYC_BOUNDS['west']) /\n",
        "                  (NYC_BOUNDS['east'] - NYC_BOUNDS['west']) * width)\n",
        "    lat_idx = int((NYC_BOUNDS['north'] - tract['lat']) /\n",
        "                  (NYC_BOUNDS['north'] - NYC_BOUNDS['south']) * height)\n",
        "\n",
        "    if 0 <= lon_idx < width and 0 <= lat_idx < height:\n",
        "        temp = risk_map[lat_idx, lon_idx]\n",
        "        if not np.isnan(temp) and temp > 0:\n",
        "            tract_temperatures.append(temp)\n",
        "            tract_vulnerabilities.append(tract['vulnerability_index'])\n",
        "\n",
        "# Calculate correlation\n",
        "if len(tract_temperatures) > 10:\n",
        "    correlation, p_value = stats.pearsonr(tract_temperatures, tract_vulnerabilities)\n",
        "    print(f\"    Correlation (temperature vs vulnerability): {correlation:.3f}\")\n",
        "    print(f\"    P-value: {p_value:.4f}\")\n",
        "\n",
        "    if p_value < 0.05:\n",
        "        print(\"    âœ“ Statistically significant relationship found!\")\n",
        "    else:\n",
        "        print(\"    âš  Relationship not statistically significant\")\n",
        "\n",
        "# Identify vulnerable hot spots\n",
        "print(\"\\n6.2 Identifying vulnerable hot spots...\")\n",
        "\n",
        "vulnerable_hotspots = demographic_data[\n",
        "    (demographic_data['vulnerability_index'] > demographic_data['vulnerability_index'].quantile(0.75))\n",
        "].copy()\n",
        "\n",
        "# Get temperatures for vulnerable areas\n",
        "vulnerable_temps = []\n",
        "for _, tract in vulnerable_hotspots.iterrows():\n",
        "    lon_idx = int((tract['lon'] - NYC_BOUNDS['west']) /\n",
        "                  (NYC_BOUNDS['east'] - NYC_BOUNDS['west']) * width)\n",
        "    lat_idx = int((NYC_BOUNDS['north'] - tract['lat']) /\n",
        "                  (NYC_BOUNDS['north'] - NYC_BOUNDS['south']) * height)\n",
        "\n",
        "    if 0 <= lon_idx < width and 0 <= lat_idx < height:\n",
        "        temp = risk_map[lat_idx, lon_idx]\n",
        "        if not np.isnan(temp):\n",
        "            vulnerable_temps.append(temp)\n",
        "\n",
        "if vulnerable_temps:\n",
        "    print(f\"    Vulnerable areas mean temperature: {np.mean(vulnerable_temps):.1f}Â°C\")\n",
        "    print(f\"    Overall mean temperature: {np.nanmean(risk_map):.1f}Â°C\")\n",
        "    print(f\"    Temperature difference: {np.mean(vulnerable_temps) - np.nanmean(risk_map):.1f}Â°C\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7: VISUALIZATIONS"
      ],
      "metadata": {
        "id": "KYXpFa4C5wSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2jFyedJAqhF",
        "outputId": "1b01700c-0700-47c6-c3da-b0a170264ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 7: CREATING VISUALIZATIONS\n",
            "================================================================================\n",
            "    âœ“ Saved: 01_temporal_prediction.png\n",
            "    âœ“ Saved: 02_risk_map.png\n",
            "    âœ“ Saved: 03_demographic_analysis.png\n",
            "    âœ“ Saved: 04_executive_summary.png\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7: CREATING VISUALIZATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Temporal Prediction Performance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "for idx, (name, res) in enumerate(results.items()):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Scatter plot\n",
        "    ax.scatter(y_predict, res['predictions'], alpha=0.5, s=10, c=y_predict, cmap='coolwarm')\n",
        "\n",
        "    # Perfect prediction line\n",
        "    min_val = min(y_predict.min(), res['predictions'].min())\n",
        "    max_val = max(y_predict.max(), res['predictions'].max())\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
        "\n",
        "    # Metrics\n",
        "    ax.text(0.05, 0.95,\n",
        "            f\"RÂ² = {res['r2']:.3f}\\nMAE = {res['mae']:.2f}Â°C\\nRMSE = {res['rmse']:.2f}Â°C\",\n",
        "            transform=ax.transAxes, va='top', fontsize=10,\n",
        "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    ax.set_xlabel('Actual 2025 Temperature (Â°C)')\n",
        "    ax.set_ylabel('Predicted 2025 Temperature (Â°C)')\n",
        "    ax.set_title(f'{name} - Temporal Prediction', fontweight='bold')\n",
        "    ax.grid(alpha=0.3)\n",
        "\n",
        "plt.suptitle('Temporal Prediction: 2024 â†’ 2025', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/01_temporal_prediction.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"    âœ“ Saved: 01_temporal_prediction.png\")\n",
        "plt.close()\n",
        "\n",
        "# 2. High-Risk Area Map\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Temperature map\n",
        "ax1 = axes[0]\n",
        "im1 = ax1.imshow(risk_map, cmap='hot', aspect='auto')\n",
        "ax1.set_title('Predicted Temperature (2025)', fontweight='bold')\n",
        "ax1.axis('off')\n",
        "cbar1 = plt.colorbar(im1, ax=ax1, fraction=0.046, pad=0.04)\n",
        "cbar1.set_label('Temperature (Â°C)', rotation=270, labelpad=20)\n",
        "\n",
        "# High-risk areas\n",
        "ax2 = axes[1]\n",
        "risk_binary = np.where(risk_map > threshold, 1, 0)\n",
        "risk_binary_masked = np.ma.masked_where(risk_binary == 0, risk_binary)\n",
        "ax2.imshow(np.ones_like(risk_map) * 0.9, cmap='gray', aspect='auto')  # Background\n",
        "im2 = ax2.imshow(risk_binary_masked, cmap='Reds', aspect='auto', alpha=0.8)\n",
        "ax2.set_title(f'High-Risk Areas (>{threshold:.1f}Â°C)', fontweight='bold')\n",
        "ax2.axis('off')\n",
        "\n",
        "plt.suptitle('Urban Heat Risk Mapping - NYC 2025', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/02_risk_map.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"    âœ“ Saved: 02_risk_map.png\")\n",
        "plt.close()\n",
        "\n",
        "# 3. Demographic Vulnerability Analysis\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "# Temperature vs Vulnerability\n",
        "ax1 = axes[0, 0]\n",
        "if tract_temperatures and tract_vulnerabilities:\n",
        "    ax1.scatter(tract_vulnerabilities, tract_temperatures, alpha=0.6, s=50, c=tract_temperatures, cmap='coolwarm')\n",
        "    z = np.polyfit(tract_vulnerabilities, tract_temperatures, 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax1.plot(tract_vulnerabilities, p(tract_vulnerabilities), \"r--\", alpha=0.8, lw=2)\n",
        "ax1.set_xlabel('Vulnerability Index')\n",
        "ax1.set_ylabel('Temperature (Â°C)')\n",
        "ax1.set_title('Temperature vs Social Vulnerability', fontweight='bold')\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Income vs Temperature\n",
        "ax2 = axes[0, 1]\n",
        "income_temps = []\n",
        "incomes = []\n",
        "for _, tract in demographic_data.iterrows():\n",
        "    lon_idx = int((tract['lon'] - NYC_BOUNDS['west']) /\n",
        "                  (NYC_BOUNDS['east'] - NYC_BOUNDS['west']) * width)\n",
        "    lat_idx = int((NYC_BOUNDS['north'] - tract['lat']) /\n",
        "                  (NYC_BOUNDS['north'] - NYC_BOUNDS['south']) * height)\n",
        "\n",
        "    if 0 <= lon_idx < width and 0 <= lat_idx < height:\n",
        "        temp = risk_map[lat_idx, lon_idx]\n",
        "        if not np.isnan(temp) and temp > 0:\n",
        "            income_temps.append(temp)\n",
        "            incomes.append(tract['median_income'])\n",
        "\n",
        "if income_temps:\n",
        "    ax2.scatter(incomes, income_temps, alpha=0.6, s=50, c=income_temps, cmap='coolwarm')\n",
        "ax2.set_xlabel('Median Income ($)')\n",
        "ax2.set_ylabel('Temperature (Â°C)')\n",
        "ax2.set_title('Income vs Temperature', fontweight='bold')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# Vulnerability components\n",
        "ax3 = axes[1, 0]\n",
        "vuln_components = ['poverty_rate', 'elderly_pct', 'minority_pct', 'no_ac_pct']\n",
        "vuln_means = [demographic_data[col].mean() for col in vuln_components]\n",
        "colors = ['#E74C3C', '#3498DB', '#2ECC71', '#F39C12']\n",
        "bars = ax3.bar(range(len(vuln_components)), vuln_means, color=colors, alpha=0.7)\n",
        "ax3.set_xticks(range(len(vuln_components)))\n",
        "ax3.set_xticklabels(['Poverty\\nRate', 'Elderly\\n%', 'Minority\\n%', 'No AC\\n%'], rotation=0)\n",
        "ax3.set_ylabel('Percentage (%)')\n",
        "ax3.set_title('Vulnerability Components - NYC Average', fontweight='bold')\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Feature importance (from best model)\n",
        "ax4 = axes[1, 1]\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    importances = best_model.feature_importances_\n",
        "    imp_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    }).sort_values('Importance', ascending=True).tail(6)\n",
        "\n",
        "    colors = plt.cm.viridis(imp_df['Importance'] / imp_df['Importance'].max())\n",
        "    ax4.barh(imp_df['Feature'], imp_df['Importance'], color=colors)\n",
        "    ax4.set_xlabel('Importance')\n",
        "    ax4.set_title('Top Features for Temperature Prediction', fontweight='bold')\n",
        "    ax4.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.suptitle('Environmental Justice Analysis', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/03_demographic_analysis.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"    âœ“ Saved: 03_demographic_analysis.png\")\n",
        "plt.close()\n",
        "\n",
        "# 4. Executive Summary Dashboard\n",
        "fig = plt.figure(figsize=(16, 10))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Title\n",
        "fig.suptitle('NYC Urban Heat Vulnerability - Executive Summary', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Risk map (large)\n",
        "ax1 = fig.add_subplot(gs[0:2, 0:2])\n",
        "im = ax1.imshow(risk_map, cmap='hot', aspect='auto')\n",
        "ax1.set_title('Predicted Temperature Map (2025)')\n",
        "ax1.axis('off')\n",
        "plt.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)\n",
        "\n",
        "# Key metrics\n",
        "ax2 = fig.add_subplot(gs[0, 2])\n",
        "ax2.axis('off')\n",
        "metrics_text = f\"\"\"\n",
        "KEY METRICS\n",
        "\n",
        "Model Performance:\n",
        "â€¢ RÂ² Score: {results[best_model_name]['r2']:.3f}\n",
        "â€¢ MAE: {results[best_model_name]['mae']:.1f}Â°C\n",
        "â€¢ RMSE: {results[best_model_name]['rmse']:.1f}Â°C\n",
        "\n",
        "Temperature Stats:\n",
        "â€¢ Mean: {np.nanmean(risk_map):.1f}Â°C\n",
        "â€¢ Max: {np.nanmax(risk_map):.1f}Â°C\n",
        "â€¢ High-risk threshold: {threshold:.1f}Â°C\n",
        "\"\"\"\n",
        "ax2.text(0.1, 0.9, metrics_text, transform=ax2.transAxes, va='top', fontsize=11,\n",
        "         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.3))\n",
        "\n",
        "# Vulnerability correlation\n",
        "ax3 = fig.add_subplot(gs[1, 2])\n",
        "if tract_temperatures and tract_vulnerabilities:\n",
        "    ax3.scatter(tract_vulnerabilities, tract_temperatures, alpha=0.5, s=20, c='#E74C3C')\n",
        "    z = np.polyfit(tract_vulnerabilities, tract_temperatures, 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax3.plot(tract_vulnerabilities, p(tract_vulnerabilities), \"r--\", alpha=0.8)\n",
        "ax3.set_xlabel('Vulnerability Index', fontsize=9)\n",
        "ax3.set_ylabel('Temperature (Â°C)', fontsize=9)\n",
        "ax3.set_title('Social Vulnerability', fontsize=10, fontweight='bold')\n",
        "ax3.grid(alpha=0.3)\n",
        "\n",
        "# Temporal change\n",
        "ax4 = fig.add_subplot(gs[2, 0])\n",
        "years = ['2024\\n(Training)', '2025\\n(Predicted)']\n",
        "temps = [np.nanmean(train_img[6]), np.nanmean(risk_map)]\n",
        "bars = ax4.bar(years, temps, color=['#3498DB', '#E74C3C'], alpha=0.7)\n",
        "ax4.set_ylabel('Mean Temperature (Â°C)')\n",
        "ax4.set_title('Temporal Change', fontsize=10, fontweight='bold')\n",
        "ax4.grid(axis='y', alpha=0.3)\n",
        "for bar, temp in zip(bars, temps):\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "             f'{temp:.1f}Â°C', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Risk distribution\n",
        "ax5 = fig.add_subplot(gs[2, 1])\n",
        "temps_flat = risk_map[~np.isnan(risk_map)]\n",
        "ax5.hist(temps_flat, bins=30, color='#2ECC71', alpha=0.7, edgecolor='black')\n",
        "ax5.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'High-risk: {threshold:.1f}Â°C')\n",
        "ax5.set_xlabel('Temperature (Â°C)')\n",
        "ax5.set_ylabel('Pixel Count')\n",
        "ax5.set_title('Temperature Distribution', fontsize=10, fontweight='bold')\n",
        "ax5.legend()\n",
        "ax5.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Policy recommendations\n",
        "ax6 = fig.add_subplot(gs[2, 2])\n",
        "ax6.axis('off')\n",
        "recommendations = \"\"\"\n",
        "POLICY RECOMMENDATIONS\n",
        "\n",
        "1. Prioritize cooling centers in\n",
        "   high-vulnerability areas\n",
        "\n",
        "2. Increase tree canopy in\n",
        "   identified hotspots\n",
        "\n",
        "3. Target heat assistance to\n",
        "   low-income neighborhoods\n",
        "\n",
        "4. Implement cool roof programs\n",
        "   in high-risk zones\n",
        "\"\"\"\n",
        "ax6.text(0.1, 0.9, recommendations, transform=ax6.transAxes, va='top', fontsize=10,\n",
        "         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.5))\n",
        "\n",
        "plt.savefig(f'{OUTPUT_DIR}/04_executive_summary.png', dpi=300, bbox_inches='tight')\n",
        "print(f\"    âœ“ Saved: 04_executive_summary.png\")\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQxc5M4KAqhG"
      },
      "source": [
        "# 8: CROSS-CITY GENERALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTGpAmTzAqhG",
        "outputId": "689c382b-1075-4065-c0b8-d4547ead239c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STEP 8: CROSS-CITY GENERALIZATION POTENTIAL\n",
            "================================================================================\n",
            "\n",
            "8.1 Model generalization analysis...\n",
            "\n",
            "    Top predictive features (universal indicators):\n",
            "      â€¢ MNDWI: 0.561\n",
            "      â€¢ Brightness: 0.294\n",
            "      â€¢ UI: 0.120\n",
            "\n",
            "    Model characteristics for generalization:\n",
            "      â€¢ Uses spectral indices (universal)\n",
            "      â€¢ Temperature prediction RÂ²: 0.841\n",
            "      â€¢ Expected transferability: MODERATE to HIGH\n",
            "\n",
            "    Recommended cities for testing:\n",
            "      â€¢ Similar climate: Boston, Philadelphia, Chicago\n",
            "      â€¢ Different climate: Phoenix, Miami, Los Angeles\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 8: CROSS-CITY GENERALIZATION POTENTIAL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n8.1 Model generalization analysis...\")\n",
        "\n",
        "# Feature importance analysis\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    importances = best_model.feature_importances_\n",
        "\n",
        "    # Top features are universal urban indicators\n",
        "    top_features = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    }).sort_values('Importance', ascending=False).head(3)\n",
        "\n",
        "    print(\"\\n    Top predictive features (universal indicators):\")\n",
        "    for _, row in top_features.iterrows():\n",
        "        print(f\"      â€¢ {row['Feature']}: {row['Importance']:.3f}\")\n",
        "\n",
        "    print(\"\\n    Model characteristics for generalization:\")\n",
        "    print(f\"      â€¢ Uses spectral indices (universal)\")\n",
        "    print(f\"      â€¢ Temperature prediction RÂ²: {results[best_model_name]['r2']:.3f}\")\n",
        "    print(f\"      â€¢ Expected transferability: MODERATE to HIGH\")\n",
        "\n",
        "    print(\"\\n    Recommended cities for testing:\")\n",
        "    print(\"      â€¢ Similar climate: Boston, Philadelphia, Chicago\")\n",
        "    print(\"      â€¢ Different climate: Phoenix, Miami, Los Angeles\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5: FINAL SUMMARY"
      ],
      "metadata": {
        "id": "iiJhY5Wh6Wsx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjxjaRc3AqhG",
        "outputId": "83b01bb4-98bf-482a-ec24-462d898bd9aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "                    ANALYSIS COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š TEMPORAL PREDICTION RESULTS:\n",
            "\n",
            "  ðŸ† Best Model: Gradient Boosting\n",
            "  ðŸ“ˆ Temporal RÂ²: 0.841\n",
            "  ðŸŽ¯ Temporal MAE: 2.19Â°C\n",
            "  ðŸ“ Temporal RMSE: 2.91Â°C\n",
            "\n",
            "ðŸŒ¡ï¸ HIGH-RISK AREAS:\n",
            "  â€¢ Threshold: 42.7Â°C (top 20%)\n",
            "  â€¢ High-risk pixels: 69,474\n",
            "  â€¢ Percentage of city: 20.0%\n",
            "\n",
            "ðŸ˜ï¸ ENVIRONMENTAL JUSTICE:\n",
            "  â€¢ Temperature-Vulnerability Correlation: -0.006\n",
            "  â€¢ Statistical significance: NO\n",
            "  â€¢ Vulnerable area temperature premium: -0.2Â°C\n",
            "\n",
            "ðŸŒ GENERALIZATION POTENTIAL:\n",
            "  â€¢ Model uses universal urban indices\n",
            "  â€¢ Expected to work in similar climate cities\n",
            "  â€¢ Requires retraining for different climates\n",
            "\n",
            "ðŸ“ All outputs saved to: ./nyc_heat_vulnerability_output/\n",
            "  1. 01_temporal_prediction.png - 2024â†’2025 prediction performance\n",
            "  2. 02_risk_map.png - High-risk area identification\n",
            "  3. 03_demographic_analysis.png - Environmental justice analysis\n",
            "  4. 04_executive_summary.png - Executive dashboard\n",
            "\n",
            "ðŸ’¡ KEY INSIGHTS FOR PHD APPLICATION:\n",
            "  â€¢ Demonstrated temporal prediction capability\n",
            "  â€¢ Identified environmental justice implications\n",
            "  â€¢ Showed potential for cross-city generalization\n",
            "  â€¢ Combined remote sensing, ML, and social data\n",
            "\n",
            "================================================================================\n",
            "âœ¨ Ready for your PhD applications! âœ¨\n",
            "ðŸŒ† Urban Heat Vulnerability Analysis Complete! ðŸ”¥\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\" \" * 20 + \"ANALYSIS COMPLETE!\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "print(\"ðŸ“Š TEMPORAL PREDICTION RESULTS:\\n\")\n",
        "print(f\"  ðŸ† Best Model: {best_model_name}\")\n",
        "print(f\"  ðŸ“ˆ Temporal RÂ²: {results[best_model_name]['r2']:.3f}\")\n",
        "print(f\"  ðŸŽ¯ Temporal MAE: {results[best_model_name]['mae']:.2f}Â°C\")\n",
        "print(f\"  ðŸ“ Temporal RMSE: {results[best_model_name]['rmse']:.2f}Â°C\")\n",
        "\n",
        "print(f\"\\nðŸŒ¡ï¸ HIGH-RISK AREAS:\")\n",
        "print(f\"  â€¢ Threshold: {threshold:.1f}Â°C (top 20%)\")\n",
        "print(f\"  â€¢ High-risk pixels: {high_risk_mask.sum():,}\")\n",
        "print(f\"  â€¢ Percentage of city: {high_risk_mask.mean()*100:.1f}%\")\n",
        "\n",
        "if correlation:\n",
        "    print(f\"\\nðŸ˜ï¸ ENVIRONMENTAL JUSTICE:\")\n",
        "    print(f\"  â€¢ Temperature-Vulnerability Correlation: {correlation:.3f}\")\n",
        "    print(f\"  â€¢ Statistical significance: {'YES' if p_value < 0.05 else 'NO'}\")\n",
        "    if vulnerable_temps:\n",
        "        print(f\"  â€¢ Vulnerable area temperature premium: {np.mean(vulnerable_temps) - np.nanmean(risk_map):.1f}Â°C\")\n",
        "\n",
        "print(f\"\\nðŸŒ GENERALIZATION POTENTIAL:\")\n",
        "print(f\"  â€¢ Model uses universal urban indices\")\n",
        "print(f\"  â€¢ Expected to work in similar climate cities\")\n",
        "print(f\"  â€¢ Requires retraining for different climates\")\n",
        "\n",
        "print(f\"\\nðŸ“ All outputs saved to: {OUTPUT_DIR}/\")\n",
        "print(f\"  1. 01_temporal_prediction.png - 2024â†’2025 prediction performance\")\n",
        "print(f\"  2. 02_risk_map.png - High-risk area identification\")\n",
        "print(f\"  3. 03_demographic_analysis.png - Environmental justice analysis\")\n",
        "print(f\"  4. 04_executive_summary.png - Executive dashboard\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ KEY INSIGHTS FOR PHD APPLICATION:\")\n",
        "print(f\"  â€¢ Demonstrated temporal prediction capability\")\n",
        "print(f\"  â€¢ Identified environmental justice implications\")\n",
        "print(f\"  â€¢ Showed potential for cross-city generalization\")\n",
        "print(f\"  â€¢ Combined remote sensing, ML, and social data\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âœ¨ Ready for your PhD applications! âœ¨\")\n",
        "print(\"ðŸŒ† Urban Heat Vulnerability Analysis Complete! ðŸ”¥\")\n",
        "print(\"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pWG_CeuAqhH"
      },
      "source": [
        "## MODEL 3 - SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci85AzV9AqhH",
        "outputId": "ae88d29d-f6e1-455d-d11d-8b4dce0e2fd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Classifier:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00     48363\n",
            "           1       1.00      0.95      0.98      5460\n",
            "\n",
            "    accuracy                           1.00     53823\n",
            "   macro avg       1.00      0.98      0.99     53823\n",
            "weighted avg       1.00      1.00      1.00     53823\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "svm = SVC(kernel='rbf', random_state=42)\n",
        "svm.fit(X_train, y_train.ravel())\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "print(\"SVM Classifier:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1BwnbV_AqhH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}